# Apache Kafka - Advanced&Annexes - AdvancedProducerConfigruation - Producer Configurations

Setting up the configurations for the Kafka Producer.

## Required configurations

- boostrap.servers
  - 'localhost:9092'
- key.serializer
  - 'str.encode'
- value.serializer
  - 'str.encode'

## Producers Acks Deep Dive

The 'acks' is basically acknowledgements.

### acks = 0 (no acks)

- No response is requested.
- If the broker goes down or an exception happens, we won't know and will lose data.
- Useful for data where it's okay to potentially lose messages.
  - Example: Metrics collections.
  - Example: Log collection

### acks = 1 (leader acks) [default]

- Leader response is requested, but replication is not a guarantee (happens in the background).
- If an acks is not received, the producer may retry.
- If the leader broker goes offline but replicas haven't replicated the data yet, we have a data loss.

### acks = all (replicas acks)

- Leader and Replicas acks requested.
- Adds latency but also safety.
- Necessary settings if the data is critical.
- `acks=all` must be used in conjuction with `min.insync.replicas`.
  - `min.insync.replicas` is a 'broker level' setting which can be overridden at 'topic level'.
  - `min.insync.replicas=2` implies that at least 2 brokers that are ISR (including leader) must respond that they have the data.
  - That means using `replication.factor=3`, `min.insync.replicas=2`, and `acks=all` can at most tolerate 1 broker being down.

## Producer Retries

- In case of transient failures, develoers are expected to handle exceptions, otherwise the data will be lost.
  - Example of transient falure: `NotEnoughReplicasException`.
- There is a `retries` setting.
  - Default value is 0.
  - Can go upto `Integer.MAX_VALUE` (Java).
- In case of retries, by default **there is a chance that messages will be sent out of order** (if a batch has failed to be sent).
- If you reply on key-based ordering, that can be an issue.
- For this, you can set the setting while controls how many produce requests can be made in parallel: `max.in.flight.requests.per.connection`.
  - Default value is 5.
  - Set it to 1 if you need to ensure oredering (may impact throughput).
- For Kafka >= 1.0.0, there is a better solution (applies to us).

## Idempotent Producer (use me)

- Problem: The Producer can introduce duplicates messages in Kafka due to network errors.
- An 'idempotent producer' doesn't introduces duplicates on network errors.
- Idempotent producers are great to guarantee a stable and safe pipeline.
- Comes with:
  - `retries = 2^31 - 1`
  - `max.in.flight.requests=5` (Kafka >= 1.1)
  - `acks=all`
- Set with by setting `"enable.idempotence", true`.

## Safe Producer

- Kafka < 0.11 have different configurations.
  - Check them if that is requried to be used.
- Kafka >= 0.11:
  - `"enable.idempotence", true` [Producer level] and `min.insync.replicas=2` [Broker / Topic level].
    - `retries = 2^31 - 1`
    - `max.in.flight.requests=5` (Kafka >= 1.1)
    - `acks=all`
- Running a 'safe producer' might impact throughput and latency, so the use may vary by case.

## Message Compression

- Producer usually sends data that is text-based, for example JSON data.
- In this case, it is important to apply compression to producer.
- Comrpression is enabled at Producer level and doesn't require any configuration change in Brokers or in the Consumers.
- `"compression.type"` can be `"None"` (Default), `gzip`, `lz4`, `snappy`.
- The compressed batch has the following advantage:
  - Much smaller producer request size (compression ratio up to 4x).
  - Faster to transer data over the network => less latency.
  - Better disk ustilization in Kafka (stored messages on disk are smaller).
- Disadvantages (very minor):
  - Producers must commit some CPU cycles to compression.
  - Consumers must commit some CPU cycles to decompression.
- Overall:
  - Consider `snappy` or `lz4` for optimal compression balance.
- Find a compression algorithm that gives you the best performance for your specific data.
- **Always use compression in production** and especially if you have high throughput.
- Consider tweaking `linger.ms` and `batch.size` to have bigger batches, and therefore more compression and high throughput.

## `linger.ms` and `batch.size`

Note: In "confluence_kafka", `batch.size` is represented with `batch.num.messages`.

- By default, Kafka tries to send records as soon as possible.
  - It will have up to 5 requests in flight, meaning up to 5 messages individually sent at the same time.
  - After this, if more messages have to be sent while others are in flight, Kafka is smart and will start batching them while they wait to send them all at once.
- This smart batching allows Kafka to increase throughput while maintaining very low latency.
- Batches have higher compression ratio so better efficieny.
- `linger.ms`: Number of milliseconds a producer is will to wait before sending a batch out (default is 0).
- By introducing some lag (for example of 5ms), we increase the chances of messages being sent together in a batch.
- So at the expense of introducing a small delay, we can increase throughput, compression and efficiency of our producer.
- If a batch is full (based on `batch.size`) before the end of `linger.ms` period, it will be sent to Kafka right away.
- `batch.size`: Maximum number of bytes that will be include in a batch (default is 16KB).
- Increasing a batch size to something like 32KB or 64KB can help increasing the compression, throughput, and efficiency of requests.
- A batch is allocate per partition, so make sure that you don't set it to a nnumber that is too high, otherwise there can be a waste of memory.
- Average batch size can be monitored using 'Kafka Producer Metrics'.

## Producer Default Partitioner And How Keys Are Hashed (theory only)

- By default, keys are hashed using "murmur2" algorithm after being converted into bytes.
- It is likely preferred to not to override the behaviour of the partitioner, but it is possible to do so using `"partitioner.class"`.
- The formula is:
  - `targetPartition = Utils.abs(Utils.murmur2(record.key())) % numPartitions;` (Java).
- This means that same key will go to the same partition, and adding partition to a topic will completely alter the formula.

## `max.block.ms` and `buffer.memory`

Note: Not found in "confluence_kafka" package.

- If the producer produces faster than the broker can take, the records will be buffered in memory.
- `buffer.memory` is the size of the send buffer (default is 33554432B [32MB]).
- That buffer will fill up over time and fill back down when the throughput to the broker increases.
- If that buffer is full, then the `.send()` method will start to block.
- `max.block.ms` is the time the `.send()` will block until throwing an exception (default is 6000).
  - Exceptions:
    - The producer has filled up its buffer.
    - The broker is not accepting any new data.
    - 60 seconds has elapsed.
- If you hit an exception hit that usually means your brokers are down or overloaded as they can't respond to requests.
