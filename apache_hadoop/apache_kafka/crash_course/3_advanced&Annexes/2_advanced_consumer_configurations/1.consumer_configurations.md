# Apache Kafka - Advanced&Annexes - AdvancedConsumerConfigruation - Consumer Configurations

- Setting up an ElasticSearch can be done by going to [bonsai.io](http://bonsai.io)
- This section also includes ElasticSearch example document.

## Delivery Semantics

- At most once
  - Offsets are committed as soon as the message batch is received. If the processing goes wrong, the message will be lost (it won't be read again).
- At least once (use me)
  - Offsets are committed after the message is processed. If the processing goes wrong, the message will be read again. This can result in duplicate processing of messages. Make your processing **idempotent** (i.e. processing again the messages won't impact your system).
- Exactly once
  - Only for kafka to kafka.

## Idempotent Consumer

Using a unique id, makes a consumer idempotent.

- Kafka Generic ID
  - `id = consumer.topic + parititon + offset`
- Specific ID
  - `id = source_specific_id`

## Consumer Poll Behaviour

- Kafka COnsumer have a "poll" model, instead of "push".
- This allows consumers to control where in the log they want to consume, how fast, and gives them ability to replay events.
- `fetch.min.bytes`:
  - Controls how much data you want to pull at least on each request.
  - Helps improving throughput and decreasing request number.
  - At the cost of latency.
  - Default is 1.
- `max.pol.records`:
  - Controls how many records to receive per poll request.
  - Increase if your messages are very small and have a lot of available RAM.
  - Good to monitor how many records are polled per request.
  - Default is 500.
- `max.paritions.fetch.bytes`:
  - Max data returned by the broker per partition.
  - If you read from 100 paritions, you'll need a lot of memory (RAM).
- `fetch.max.bytes`:
  - Maximum data returned for each fetch request (covers multiple partitions).
  - The consumer performs multiple fetches in parallel.

Note: Defaults are good enough. Only change when throughput needs to be changed.

## Consumer Offset Commits Strategies

- There are two most common patterns for committing offsets in a consumer application.
- `enable.auto.commit`:
  - Easy:
    - Set as true, by default.
    - Synchronous processing of batches.
    - With auto-commit, offsets are automatically committed at a regular interval everytime `.poll()` is called.
    - `auto.commit.interval.ms` (Default is 5000).
    - Not using synchronous process will make it have a 'at-most-once' behaviour because offsets will be committed before your data is processed.
  - Medium:
    - Set as false.
    - Manual commit of offsets.
    - Control when you commit offsets and the conditions of when to commit.
    - Example: Accumulating records into a buffer and then flushing the buffer to a database + committing offsets then.

## Consumer Offset Reset Behaviour

- A consumer is expected to read from a log continuously.
- Kafka retention perioud is 7 days.
- If consumer goes down for more than 7 days, the offsets can become invalid.
- The behaviour for the consumer is to use:
  - `auto.offset.reset=latest`
    - `latest`: Will read from the end of the log.
    - `earliest`: Will read from the start of the log.
    - `none`: Will throw exception if no offset is found.
- Consumer offset can be lost:
  - Before Kafka 2.0, if a consumer hasn't read new data in 1 day.
  - After Kafka 2.0, if a consumer hasn't read new data in 7 day.
  - Controlled via `offset.retention.minutes`.
- Use `kafka-consumer-groups` to set offset.
- Set a proper data and offset retention period.
