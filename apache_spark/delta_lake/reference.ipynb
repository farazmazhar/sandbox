{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake\n",
    "\n",
    "Delta Lake is an open source storage layer that brings reliability to data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs.\n",
    "\n",
    "This notebook contains demostration of some basic delta lake functionality.\n",
    "\n",
    "### Prerequisite\n",
    "\n",
    "To use Delta Lake, first you need **Apache Spark** to be installed.\n",
    "\n",
    "### Installation\n",
    "\n",
    "To install Delta Lake, go to [Delta Core repository on Maven](https://mvnrepository.com/artifact/io.delta/delta-core). Make sure to download the jar file for the correct Scala version. After downloading the jar file, copy it into the `$SPARK_HOME/jars` folder.\n",
    "\n",
    "To check your Scala version, execute following command in your terminal.\n",
    "\n",
    "```sh\n",
    "pyspark --version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    "\n",
    "SparkSession with the following configuration integrates the Delta Lake layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"delta_lake\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV data\n",
    "\n",
    "I'm using CSV files to start so that different ways of creating Delta Tables can be demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.csv('movie.csv', sep=',', inferSchema=True, header=True)\n",
    "ratings_df = spark.read.csv('ratings.csv', sep=',', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|196   |242    |3     |881250949|\n",
      "|186   |302    |3     |891717742|\n",
      "|22    |377    |1     |878887116|\n",
      "|244   |51     |2     |880606923|\n",
      "|166   |346    |1     |886397596|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Delta Table\n",
    "\n",
    "First method of creating Delta Table is to write the Spark DataFrame using `.format('delta')`. This will write data in Parquet file format but with a folder called `_delta_log` which will contain log for all the operations that will occur on this data. This allows you to keep track of the changes and travel back in time to a specific version of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.write.format('delta').save('movies_delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Delta Table (cont.)\n",
    "\n",
    "An alternative way of creating Delta Tables is to using function `convertToDelta` on data that is already in the parquet format. This function will create the `_delta_log` folder for the given data.\n",
    "\n",
    "Function `isDeltaTable` returns either `True` or `False` depending on whether the given location contains a Delta Table or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.write.format('parquet').save('ratings_parquet')\n",
    "DeltaTable.isDeltaTable(spark, 'ratings_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeltaTable.convertToDelta(spark, \"parquet.`ratings_parquet`\")\n",
    "DeltaTable.isDeltaTable(spark, 'ratings_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Delta Table\n",
    "\n",
    "First method of reading a Delta Table is by using function `forPath`. This function creates a Delta Table from the given path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dt = DeltaTable.forPath(spark, 'movies_delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Delta Table (cont.)\n",
    "\n",
    "Alternatively, Delta Tables can be read from the table name used to store the table in memory.\n",
    "\n",
    "```py\n",
    "movies_df.write.format('delta').option('save', 'movies_delta').saveAsTable('movies')\n",
    "movies_dt_by_name = DeltaTable.forName(spark, 'movies')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table\n",
    "\n",
    "On the surface, Delta Tables lack the methods and functionality of the DataFrames since the API contains very limited functionality. Delta Tables, however, can be converted into DataFrame for querying purposes or can be registered as a table which allows Hive compatible queries to be executed on them using the function `SparkSession.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dt.toDF().registerTempTable('movies_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|movieId|title                             |genres                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                                     |\n",
      "+-------+----------------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM movies_dt').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table (cont.)\n",
    "\n",
    "Let's create a DataFrame for an update. I am keeping IDs of all the new entries below zero so changes be observed when sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------------------------------------+\n",
      "|movieId|title            |genres                                            |\n",
      "+-------+-----------------+--------------------------------------------------+\n",
      "|-1     |Life of Faraz    |Adventure|Comedy|Thriller|Drama|Sci-Fi            |\n",
      "|-2     |A Very Cool Movie|Children|Fantasy                                  |\n",
      "|1      |Toy Story (1995) |Adventure|Animation|Children|Comedy|Fantasy|Sci-Fi|\n",
      "|2      |Jumanji (1995)   |Comedy|Romance|Adventure                          |\n",
      "+-------+-----------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update_movies_df = spark.createDataFrame([\n",
    "    [-1, 'Life of Faraz', 'Adventure|Comedy|Thriller|Drama|Sci-Fi'],\n",
    "    [-2, 'A Very Cool Movie', 'Children|Fantasy'],\n",
    "    [1, 'Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy|Sci-Fi'],\n",
    "    [2, 'Jumanji (1995)', 'Comedy|Romance|Adventure']\n",
    "], ['movieId', 'title', 'genres'])\n",
    "\n",
    "update_movies_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table: Upsert\n",
    "\n",
    "The starting point of the upsert operation is the function `merge`. It accepts two parameters one being the source which accepts a DataFrame and the other being condition which act like a WHERE clause. This function return DeltaMergeBuilder which provides multiple upsert related functions.\n",
    "\n",
    "```md\n",
    "Parameters:\t\n",
    "    source (pyspark.sql.DataFrame) – Source DataFrame\n",
    "    condition (str or pyspark.sql.Column) – Condition to match sources rows with the Delta table rows.\n",
    "```\n",
    "\n",
    "First two are as following:\n",
    "\n",
    "- Function `whenMatchedUpdate` update rows when the merge conditions are met.\n",
    "```md\n",
    "    Parameters:\t\n",
    "        condition (str or pyspark.sql.Column) – Optional condition of the update\n",
    "        set (dict with str as keys and str or pyspark.sql.Column as values) – Defines the rules of setting the values of columns that need to be updated. Note: This param is required. Default value None is present to allow positional args in same order across languages.\n",
    "```\n",
    "\n",
    "- Function `whenNotMatchedInsert` inserts new rows when the merge conditions are not met.\n",
    "```md\n",
    "    Parameters:\t\n",
    "        condition (str or pyspark.sql.Column) – Optional condition of the insert\n",
    "        values (dict with str as keys and str or pyspark.sql.Column as values) – Defines the rules of setting the values of columns that need to be updated. Note: This param is required. Default value None is present to allow positional args in same order across languages.\n",
    "```\n",
    "\n",
    "**Note:** Since the function `merge` returns DeltaMergeBuilder, it requires the function `execute` to execute the upsert operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dt.alias('movies_dt') \\\n",
    "    .merge(source=update_movies_df.alias('update'),\n",
    "           condition='movies_dt.movieId = update.movieId') \\\n",
    "    .whenMatchedUpdate(condition='movies_dt.movieId = 1',\n",
    "                       set={'title': F.upper('movies_dt.title')}) \\\n",
    "    .whenNotMatchedInsert(condition='update.movieId = -1',\n",
    "                          values={'movieId': 'update.movieId',\n",
    "                                  'title': 'update.title',\n",
    "                                  'genres': 'update.genres'}) \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table (cont.)\n",
    "\n",
    "You can query the results of the upsert operation using Hive compatible queries in the function `SparkSession.sql`.\n",
    "\n",
    "One point to note is that unlike DataFrames, Delta Tables get written to disk and memory as they get updated without the need of explicitly writing or registering them. In the next cell, I queried using same table name I registered earlier without making any modifications and I still got updated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+-------------------------------------------+\n",
      "|movieId|title                   |genres                                     |\n",
      "+-------+------------------------+-------------------------------------------+\n",
      "|-1     |Life of Faraz           |Adventure|Comedy|Thriller|Drama|Sci-Fi     |\n",
      "|1      |TOY STORY (1995)        |Adventure|Animation|Children|Comedy|Fantasy|\n",
      "|2      |Jumanji (1995)          |Adventure|Children|Fantasy                 |\n",
      "|3      |Grumpier Old Men (1995) |Comedy|Romance                             |\n",
      "|4      |Waiting to Exhale (1995)|Comedy|Drama|Romance                       |\n",
      "+-------+------------------------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM movies_dt ORDER BY movieId').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table: History\n",
    "\n",
    "Since Delta Tables maintain a log, peaking into the history is just a matter of calling the function `history`.\n",
    "\n",
    "Although, the fucntion returns a DataFrame with a lot of columns, I've decided to show only a few relavent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 1                                                                                                                                                                                 \n",
      " timestamp           | 2020-11-29 07:44:07.591                                                                                                                                                           \n",
      " operation           | MERGE                                                                                                                                                                             \n",
      " operationParameters | [predicate -> (CAST(movies_dt.`movieId` AS BIGINT) = update.`movieId`), updatePredicate -> (movies_dt.`movieId` = 1), insertPredicate -> (update.`movieId` = CAST(-1 AS BIGINT))] \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 0                                                                                                                                                                                 \n",
      " timestamp           | 2020-11-29 07:43:37.635                                                                                                                                                           \n",
      " operation           | WRITE                                                                                                                                                                             \n",
      " operationParameters | [mode -> ErrorIfExists, partitionBy -> []]                                                                                                                                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_dt.history().select('version', 'timestamp', 'operation', 'operationParameters').show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table: Upsert (cont.)\n",
    "\n",
    "The merge builder provides a few more upsert functions which are as following:\n",
    "\n",
    "- Function `whenMatchedDelete` deletes rows where merge conditions are met.\n",
    "```md\n",
    "    Parameters:\t\n",
    "        condition (str or pyspark.sql.Column) – Optional condition of the delete\n",
    "```\n",
    "\n",
    "- Function `whenMatchedUpdateAll` updates all rows where merge conditions are met.\n",
    "```md\n",
    "    Parameters:\t\n",
    "        condition (str or pyspark.sql.Column) – Optional condition of the delete\n",
    "```\n",
    "\n",
    "- Function `whenNotMatchedInsertAll` insert all rows where merge conditions are not met.\n",
    "```md\n",
    "    Parameters:\t\n",
    "        condition (str or pyspark.sql.Column) – Optional condition of the delete\n",
    "```\n",
    "\n",
    "**Note:** Each `whenMatched` clause can have an optional condition. However, if there are two `whenMatched clauses`, then the first one must have a condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dt.alias('movies_dt') \\\n",
    "    .merge(source=update_movies_df.alias('update'),\n",
    "           condition='movies_dt.movieId = update.movieId') \\\n",
    "    .whenMatchedDelete(condition='movies_dt.movieId = 1') \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+--------------------------------------+\n",
      "|movieId|title                   |genres                                |\n",
      "+-------+------------------------+--------------------------------------+\n",
      "|-2     |A Very Cool Movie       |Children|Fantasy                      |\n",
      "|-1     |Life of Faraz           |Adventure|Comedy|Thriller|Drama|Sci-Fi|\n",
      "|2      |Jumanji (1995)          |Comedy|Romance|Adventure              |\n",
      "|3      |Grumpier Old Men (1995) |Comedy|Romance                        |\n",
      "|4      |Waiting to Exhale (1995)|Comedy|Drama|Romance                  |\n",
      "+-------+------------------------+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM movies_dt ORDER BY movieId').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 2                                                                                                                                                                                 \n",
      " timestamp           | 2020-11-29 07:44:18.637                                                                                                                                                           \n",
      " operation           | MERGE                                                                                                                                                                             \n",
      " operationParameters | [predicate -> (CAST(movies_dt.`movieId` AS BIGINT) = update.`movieId`), deletePredicate -> (movies_dt.`movieId` = 1)]                                                             \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 1                                                                                                                                                                                 \n",
      " timestamp           | 2020-11-29 07:44:07.591                                                                                                                                                           \n",
      " operation           | MERGE                                                                                                                                                                             \n",
      " operationParameters | [predicate -> (CAST(movies_dt.`movieId` AS BIGINT) = update.`movieId`), updatePredicate -> (movies_dt.`movieId` = 1), insertPredicate -> (update.`movieId` = CAST(-1 AS BIGINT))] \n",
      "-RECORD 2------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 0                                                                                                                                                                                 \n",
      " timestamp           | 2020-11-29 07:43:37.635                                                                                                                                                           \n",
      " operation           | WRITE                                                                                                                                                                             \n",
      " operationParameters | [mode -> ErrorIfExists, partitionBy -> []]                                                                                                                                        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_dt.history(limit=3) \\\n",
    "    .select('version', 'timestamp', 'operation', 'operationParameters').show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table: Update\n",
    "\n",
    "The function `update` updates the Delta Table based on given condition, applying provided operation set.\n",
    "```md\n",
    "Parameters:\t\n",
    "    condition (str or pyspark.sql.Column) – Optional condition of the update\n",
    "    set (dict with str as keys and str or pyspark.sql.Column as values) – Defines the rules of setting the values of columns that need to be updated. Note: This param is required. Default value None is present to allow positional args in same order across languages.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dt \\\n",
    "    .update(condition='movieId = 2',\n",
    "            set={'genres': F.lit('Adventure|Children|Fantasy')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------+--------------------------------------+\n",
      "|movieId|title                   |genres                                |\n",
      "+-------+------------------------+--------------------------------------+\n",
      "|-2     |A Very Cool Movie       |Children|Fantasy                      |\n",
      "|-1     |Life of Faraz           |Adventure|Comedy|Thriller|Drama|Sci-Fi|\n",
      "|2      |Jumanji (1995)          |Adventure|Children|Fantasy            |\n",
      "|3      |Grumpier Old Men (1995) |Comedy|Romance                        |\n",
      "|4      |Waiting to Exhale (1995)|Comedy|Drama|Romance                  |\n",
      "+-------+------------------------+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM movies_dt ORDER BY movieId').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 3                                                                                                                                                                                 \n",
      " timestamp           | 2020-11-29 07:44:24.622                                                                                                                                                           \n",
      " operation           | UPDATE                                                                                                                                                                            \n",
      " operationParameters | [predicate -> (movieId#1015 = 2)]                                                                                                                                                 \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 2                                                                                                                                                                                 \n",
      " timestamp           | 2020-11-29 07:44:18.637                                                                                                                                                           \n",
      " operation           | MERGE                                                                                                                                                                             \n",
      " operationParameters | [predicate -> (CAST(movies_dt.`movieId` AS BIGINT) = update.`movieId`), deletePredicate -> (movies_dt.`movieId` = 1)]                                                             \n",
      "-RECORD 2------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 1                                                                                                                                                                                 \n",
      " timestamp           | 2020-11-29 07:44:07.591                                                                                                                                                           \n",
      " operation           | MERGE                                                                                                                                                                             \n",
      " operationParameters | [predicate -> (CAST(movies_dt.`movieId` AS BIGINT) = update.`movieId`), updatePredicate -> (movies_dt.`movieId` = 1), insertPredicate -> (update.`movieId` = CAST(-1 AS BIGINT))] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_dt.history(limit=3) \\\n",
    "    .select('version', 'timestamp', 'operation', 'operationParameters').show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table: Delete\n",
    "\n",
    "The function `delete` deletes rows from the the Delta Table based on given condition.\n",
    "```md\n",
    "Parameters:\t\n",
    "    condition (str or pyspark.sql.Column) – Optional condition of the update\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|27279   |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) FROM movies_dt').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dt.delete(condition='movieId < 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|27277   |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) FROM movies_dt').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+--------------------------+\n",
      "|movieId|title                             |genres                    |\n",
      "+-------+----------------------------------+--------------------------+\n",
      "|2      |Jumanji (1995)                    |Adventure|Children|Fantasy|\n",
      "|3      |Grumpier Old Men (1995)           |Comedy|Romance            |\n",
      "|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance      |\n",
      "|5      |Father of the Bride Part II (1995)|Comedy                    |\n",
      "|6      |Heat (1995)                       |Action|Crime|Thriller     |\n",
      "+-------+----------------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM movies_dt ORDER BY movieId').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 4                                                                                                                     \n",
      " timestamp           | 2020-11-29 07:44:31.314                                                                                               \n",
      " operation           | DELETE                                                                                                                \n",
      " operationParameters | [predicate -> [\"(`movieId` < 0)\"]]                                                                                    \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 3                                                                                                                     \n",
      " timestamp           | 2020-11-29 07:44:24.622                                                                                               \n",
      " operation           | UPDATE                                                                                                                \n",
      " operationParameters | [predicate -> (movieId#1015 = 2)]                                                                                     \n",
      "-RECORD 2------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 2                                                                                                                     \n",
      " timestamp           | 2020-11-29 07:44:18.637                                                                                               \n",
      " operation           | MERGE                                                                                                                 \n",
      " operationParameters | [predicate -> (CAST(movies_dt.`movieId` AS BIGINT) = update.`movieId`), deletePredicate -> (movies_dt.`movieId` = 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_dt.history(limit=3) \\\n",
    "    .select('version', 'timestamp', 'operation', 'operationParameters').show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Delta Table: Vacuum\n",
    "\n",
    "Recursively delete files and directories in the table that are not needed by the table for maintaining older versions up to the given retention threshold. This method will return an empty DataFrame on successful completion.\n",
    "\n",
    "```md\n",
    "Parameters:\t\n",
    "    retentionHours – Optional number of hours retain history. If not specified, then the default retention period of 168 hours (7 days) will be used.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_dt.vacuum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Delta Table: Time Travel\n",
    "\n",
    "Since Delta Tables keep a log of every operation, options `timestampAsOf` and `versionAsOf` can be used to travel back in time and get data from specific Delta Table versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp of second last version.\n",
    "timestamp_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", '2020-11-29 07:44:24.622') \\\n",
    "    .load(\"movies_delta\")\n",
    "\n",
    "# Version after first modification.\n",
    "version_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 1) \\\n",
    "    .load(\"movies_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|     -2|   A Very Cool Movie|    Children|Fantasy|\n",
      "|     -1|       Life of Faraz|Adventure|Comedy|...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp_df.orderBy('movieId').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|     -1|       Life of Faraz|Adventure|Comedy|...|\n",
      "|      1|    TOY STORY (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "version_df.orderBy('movieId').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Delta Table: symlink_format_manifest\n",
    "\n",
    "This function generates manifests in symlink format for Presto and Athena read support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dt.generate(mode='symlink_format_manifest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Delta Lake introduces bunch of new features and functionalities in Apache Spark which can be helpful in a lot of applications, a few of which have been mentioned in this notebook. Delta Lake, with introducing new features, sometimes require certain tasks to be done slightly differently than Spark DataFrames.\n",
    "\n",
    "Please use the following references for more information.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Delta Lake documentation](https://docs.delta.io/latest/delta-intro.html)\n",
    "- [Delta Lake Python API documentation](https://docs.delta.io/latest/api/python/index.html)\n",
    "- [Delta Core Maven Repository](https://mvnrepository.com/artifact/io.delta/delta-core)\n",
    "- [Apache Spark documentation](https://spark.apache.org/docs/latest/)\n",
    "- [Apache Spark Python API documentation](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "\n",
    "## Data Source\n",
    "\n",
    "- [MovieLens-20M by GroupLens](https://grouplens.org/datasets/movielens/20m/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
